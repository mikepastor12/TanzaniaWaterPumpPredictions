{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8854461",
   "metadata": {},
   "source": [
    "#  Tanzania Water Pump Failure Predictions - Competition\n",
    "\n",
    "## <font color=green>Implement a Predictive Model to help solve this problem and submit to DRIVENDATA competition</font>\n",
    "\n",
    "\n",
    "\n",
    ">  **This is a WORK-IN-PROGRESS -   10/26/2022 - please check back for the final version...**\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Can you predict which water pumps are faulty?\n",
    "\n",
    "Using data from Taarifa and the Tanzanian Ministry of Water, \n",
    "can you predict which pumps are functional, which need some repairs, \n",
    "and which don't work at all? Predict one of these three classes\n",
    "based on a number of variables about what kind of pump is operating, \n",
    "when it was installed, and how it is managed. \n",
    "\n",
    "A smart understanding of which waterpoints will fail can improve maintenance operations \n",
    "and ensure that clean, potable water is available to communities across Tanzania.\n",
    "\n",
    "...\n",
    "\n",
    "Hope this simple example of an XGBoost predictive model helps!\n",
    "\n",
    "All the best,\n",
    "Mike Pastor\n",
    "\n",
    "\n",
    "**Competition Home Page:**  \n",
    "[DRIVENDATA - Data science competitions\n",
    "to build a better world](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04a294",
   "metadata": {},
   "source": [
    "#  First, let's load all of the necessary Python libraries\n",
    "#    ( Note: these need to be installed on the Python instance running the NoteBook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1202cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############  Starting up...  - Current Time = 11:51:23\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd # load and manipulate data and for One-Hot Encoding\n",
    "import numpy as np # calculate the mean and standard deviation\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "\n",
    "import xgboost as xgb # XGBoost stuff\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split  data into training and testing sets\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer # for scoring during cross validation\n",
    "from sklearn.model_selection import GridSearchCV # cross validation\n",
    "from sklearn.metrics import confusion_matrix # creates a confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#  Track the overall time for training & submission preparation\n",
    "#\n",
    "global_now = datetime.now()\n",
    "global_current_time = global_now.strftime(\"%H:%M:%S\")\n",
    "print(\"##############  Starting up...  - Current Time =\", global_current_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a6e89",
   "metadata": {},
   "source": [
    "#  Now, let's open the Datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47156000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount_tsh           float64\n",
      "gps_height             int64\n",
      "longitude            float64\n",
      "latitude             float64\n",
      "population             int64\n",
      "construction_year      int64\n",
      "dtype: object\n",
      "Datasets Loaded & Merged...   Y - X  (59400, 1) (59400, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load TRAIN datasets ###########################################################\n",
    "#\n",
    "train_X = pd.read_csv('./TrainingSetValues.csv')\n",
    "train_Y = pd.read_csv('./TrainingSetLABELS.csv')\n",
    "\n",
    "# Merge the Y onto the X to be sure they are matched\n",
    "#  This adds 'status_group' LABEL onto each X example row\n",
    "#\n",
    "train_X = train_X.merge( train_Y, how='inner', on='id' )\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# replacing Y  string values with numerics\n",
    "#    This is becomes part of our Tanzania Data Dictionary\n",
    "#\n",
    "train_X['status_group'].replace(\\\n",
    "    ['functional', 'functional needs repair', 'non functional'],\\\n",
    "    [0, 1, 2], inplace=True)\n",
    "\n",
    "# Used the synchronized - quantitative  LABEL for Y\n",
    "train_Y = pd.DataFrame( train_X['status_group'], columns=['status_group'] )\n",
    "\n",
    "\n",
    "########################################################\n",
    "#  Also  Load the TEST dataset which is used for the Submission preparation\n",
    "#\n",
    "test_X = pd.read_csv('./TestSetValues.csv')\n",
    "\n",
    "\n",
    "#  Take a subset for testing\n",
    "train_X = train_X[ [    'amount_tsh', 'gps_height', 'longitude', 'latitude', 'population', 'construction_year' ] ]\n",
    "#                         # 'latitude', 'longitude', 'basin', 'region_code', 'district_code', 'population', \\\n",
    "#                         # 'scheme_name', 'scheme_management', 'construction_year', 'quantity',   'payment', \\\n",
    "#                         # 'extraction_type', 'extraction_type_class', 'source', 'source_type', \\\n",
    "#                         'management', 'waterpoint_type'  ]  ]\n",
    "\n",
    "\n",
    "# Save IDs for Submission file preparation below\n",
    "test_X_Saved_IDs = test_X[ [  'id' ] ]\n",
    "\n",
    "test_X = test_X[ [      'amount_tsh', 'gps_height', 'longitude', 'latitude', 'population', 'construction_year' ] ]\n",
    "                        # 'latitude', 'longitude', 'basin', 'region_code', 'district_code', 'population', \\\n",
    "                        # 'scheme_name', 'scheme_management', 'construction_year', 'quantity',   'payment', \\\n",
    "                        # 'extraction_type', 'extraction_type_class', 'source', 'source_type', \\\n",
    "                        # 'management', 'waterpoint_type'  ]  ]\n",
    "\n",
    "\n",
    "print( train_X.dtypes )\n",
    "\n",
    "\n",
    "print('Datasets Loaded & Merged...   Y - X ', train_Y.shape, train_X.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60524345",
   "metadata": {},
   "source": [
    "# Okay, now let's massage our data into a readable format...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abdef825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before get_dummies Train_X == (59400, 6)\n",
      "before get_dummies test_X == (14850, 6)\n",
      "AFTER  Train_X == (59400, 6)\n",
      "AFTER test_X == (14850, 6)\n",
      "Categorical variables (columns) are: []\n",
      "Numerical variables (columns) are: ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'population', 'construction_year']\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#   Let's work on the train_X  CATEGORICAL columns\n",
    "#       - Same for TEST\n",
    "\n",
    "print( 'before get_dummies Train_X ==', train_X.shape )\n",
    "print( 'before get_dummies test_X ==', test_X.shape )\n",
    "train_X = pd.get_dummies( train_X )\n",
    "test_X = pd.get_dummies( test_X )\n",
    "\n",
    "# Align the number of features across test sets based on train dataset\n",
    "train_X, test_X = train_X.align(test_X, join='left', axis=1, fill_value=0 )\n",
    "\n",
    "print( 'AFTER  Train_X ==', train_X.shape )\n",
    "print( 'AFTER test_X ==', test_X.shape )\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# define list  categorical variables (columns)\n",
    "categorical = list(train_X.select_dtypes('object').columns)\n",
    "print(f\"Categorical variables (columns) are: {categorical}\")\n",
    "\n",
    "# define list   numerical variables (columns)\n",
    "numerical = list(train_X.select_dtypes('number').columns)\n",
    "print(f\"Numerical variables (columns) are: {numerical}\")\n",
    "\n",
    "# Save the list of Features here for later reporting use\n",
    "#\n",
    "feature_list = train_X.columns\n",
    "\n",
    "\n",
    "#################################################################\n",
    "#   Clean ampersands from  2 features\n",
    "#\n",
    "#  Bad characters in feature name:   'installer', 'funder',\n",
    "\n",
    "#  train_X.installer = train_X.installer.str.replace('\\W', ' ', regex=True)\n",
    "#   train_X.funder = train_X.funder.str.replace('\\W', ' ', regex=True)\n",
    "\n",
    "#   print( '#####   Replaced special characters in features ' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9cbc3",
   "metadata": {},
   "source": [
    "#  Now split the TRAINING dataset into a VALIDATION set also\n",
    "##   This allows us to take measurements with a formal submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37bbfc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********  Train X  set is 47520 rows  and Validation set is 11880 rows \n",
      "***********  Train Y  set is 47520 rows  and Validation set is 11880 rows \n"
     ]
    }
   ],
   "source": [
    "\n",
    "####################################################################################\n",
    "# Create a split Training and new Validation from the  merged TRAIN dataset\n",
    "#\n",
    "X_TRAIN_SplitDF, X_VALIDATE_SplitDF,  Y_TRAIN_SplitDF, Y_VALIDATE_SplitDF =\\\n",
    "    train_test_split( train_X,  train_Y, test_size=0.20 )\n",
    "\n",
    "print(f\"***********  Train X  set is {len(X_TRAIN_SplitDF)} rows  and Validation set is {len(X_VALIDATE_SplitDF)} rows \")\n",
    "print(f\"***********  Train Y  set is {len(Y_TRAIN_SplitDF)} rows  and Validation set is {len(Y_VALIDATE_SplitDF)} rows \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9633ac9",
   "metadata": {},
   "source": [
    "# Let's start assembling the XGBoost tree model...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a84ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STARTING   XGBoost  Model... \n",
      "Model is Fit and ready to predict...  -  XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
      "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
      "              predictor='auto', random_state=0, reg_alpha=0, ...)\n"
     ]
    }
   ],
   "source": [
    "################################################################################################\n",
    "\n",
    "print(\" STARTING   XGBoost  Model... \")\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor       # Regression version of XGBoost\n",
    "\n",
    "#  enable_categorical=False\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit( X_TRAIN_SplitDF, Y_TRAIN_SplitDF )\n",
    "\n",
    "print( \"Model is Fit and ready to predict...  - \",model )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c12fe93",
   "metadata": {},
   "source": [
    "#  Now let's start our performance evaluation...\n",
    "#  How well are we really predicting here??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b70e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#  Performance & Evaluation of the Model\n",
    "#\n",
    "\n",
    "\n",
    "def AssessErrors(model,X,y):\n",
    "\n",
    "    # Use X to predict on 'model'\n",
    "\n",
    "    print(\"\\n##### PREDICT from X on model...\", )\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # print(\"\\n##### PREDICTED:  \", predictions )\n",
    "\n",
    "    # yhat = np.argmax(predictions, axis=1)\n",
    "    yhat = predictions\n",
    "\n",
    "    # print(\"yhat COUNT == \", len( yhat ))\n",
    "    #  print(\"y COUNT == \", y.shape )\n",
    "    # print(\"y head == \", y.head)\n",
    "\n",
    "    #  For Each Y ...\n",
    "    #    Also compute the research statistics - truePositive, etc.\n",
    "    #\n",
    "    ctr = 0\n",
    "    origPositivityRate = predPositivityRate = errors = 0\n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    df = y.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "    for index, yp in y.iterrows():\n",
    "\n",
    "        if ( yp['status_group'] == yhat[ctr] ):\n",
    "            if yp['status_group'] > 0:\n",
    "                truePositive = truePositive + 1\n",
    "            else:\n",
    "                trueNegative = trueNegative + 1\n",
    "        else:\n",
    "            errors = errors + 1   # overall error rate\n",
    "\n",
    "            if yp['status_group'] > 0:\n",
    "                falsePositive = falsePositive + 1\n",
    "            else:\n",
    "                falseNegative = falseNegative + 1\n",
    "\n",
    "        if (yp['status_group'] > 0):\n",
    "            origPositivityRate = origPositivityRate + 1\n",
    "        if ( yhat[ctr] > 0 ):\n",
    "            predPositivityRate = predPositivityRate + 1\n",
    "\n",
    "        ctr = ctr + 1\n",
    "\n",
    "    print('Precision/Recall Stats...TP first\\n')\n",
    "    print (  truePositive, falsePositive)\n",
    "    print(  falseNegative, trueNegative)\n",
    "\n",
    "    precision = truePositive / ( truePositive + falsePositive )\n",
    "    recall = truePositive / (truePositive + falseNegative )\n",
    "    print( 'PRECISION == ', precision, '  -  RECALL == ', recall )\n",
    "    print('ORIG Positivity % == ', (origPositivityRate / ctr) , ' -   PREDICTION Positivity % == ', predPositivityRate / ctr  )\n",
    "\n",
    "    #\n",
    "    # doo = yhat != y[:,0]\n",
    "    #  get only the predictions (yhat) that don't match Groundtruth (y)\n",
    "    # alt_idxs = np.where(yhat != y[:,0])[0]\n",
    "    # alt_idxs = np.where(yhat != y)\n",
    "    # alt_idxs = np.where(yhat == y[:, 0])[0]\n",
    "\n",
    "    # Return total number of errors\n",
    "    return(errors)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c65e2",
   "metadata": {},
   "source": [
    "# First let's test Training against itself...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06d0daee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### PREDICT from X on model...\n",
      "Precision/Recall Stats...TP first\n",
      "\n",
      "12851 8782\n",
      "2961 22926\n",
      "PRECISION ==  0.5940461332223917   -  RECALL ==  0.8127371616493803\n",
      "ORIG Positivity % ==  0.455239898989899  -   PREDICTION Positivity % ==  0.3477483164983165\n",
      "TRAIN DATASET provides 11743 errors out of 47520 predictions  \n",
      "for 0.752883 Accuracy and 0.247117 ERROR Rate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################################################\n",
    "#  Calculate our ERROR rate on the TRAINING dataset\n",
    "#\n",
    "tmp1 = AssessErrors(model,X_TRAIN_SplitDF,Y_TRAIN_SplitDF)\n",
    "len1 = len(X_TRAIN_SplitDF)\n",
    "print( f\"TRAIN DATASET provides {tmp1} errors out of {len1} predictions  \\nfor { round(((len1-tmp1)/len1), 6) } Accuracy and { round((tmp1/len1), 6) } ERROR Rate\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595b0cc",
   "metadata": {},
   "source": [
    "#  Do the same for our VALIDATION dataset...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73932837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### PREDICT from X on model...\n",
      "Precision/Recall Stats...TP first\n",
      "\n",
      "2913 2595\n",
      "1016 5356\n",
      "PRECISION ==  0.5288671023965141   -  RECALL ==  0.7414100279969458\n",
      "ORIG Positivity % ==  0.4636363636363636  -   PREDICTION Positivity % ==  0.3466329966329966\n",
      "VALIDATION DEV SET provides 3611 errors out of 11880 predictions \n",
      "for 0.696044 Accuracy and 0.303956 ERROR Rate\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "#  Calculate our internal ERROR rate on the VALIDATION dataset\n",
    "#\n",
    "\n",
    "tmp1 = AssessErrors(model,X_VALIDATE_SplitDF,Y_VALIDATE_SplitDF)\n",
    "len1 = len(X_VALIDATE_SplitDF)\n",
    "print( f\"VALIDATION DEV SET provides {tmp1} errors out of {len1} predictions \\nfor { round(((len1-tmp1)/len1), 6) } Accuracy and { round((tmp1/len1), 6) } ERROR Rate\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63130f70",
   "metadata": {},
   "source": [
    "#  Now we can predict on the provided TEST dataset & prepare the competition Submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c11fb733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### PREDICT from  TEST   on model for Submission file...\n",
      "TEST   PREDICTION  Yields  shape ==   (14850, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#########################################################################\n",
    "#  PREDICT from the TEST set now to prepare the Submission file\n",
    "#\n",
    "\n",
    "print(\"\\n##### PREDICT from  TEST   on model for Submission file...\")\n",
    "predictions = model.predict(test_X)  # prediction\n",
    "\n",
    "predNumpy = pd.DataFrame( predictions )\n",
    "print(\"TEST   PREDICTION  Yields  shape ==  \", predNumpy.shape )\n",
    "# print(f\" PREDICTION Yields ==  \\n\", predNumpy.head() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48427c32",
   "metadata": {},
   "source": [
    "#  Now write the actual Submission file in the specified format...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "546ed50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction POSITIVITY Count ==  5025   Rate ==  0.3383838383838384\n",
      "\n",
      "Wrote Submission file (tanzania_submission.csv) with Shape ==  (14850, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#####################################################################\n",
    "# Now write the submission file in the required format\n",
    "#\n",
    "submissionDF = pd.DataFrame(columns =['id', 'status_group'])\n",
    "\n",
    "# submissionDF['id'] = test_X['id']\n",
    "submissionDF['id'] = test_X_Saved_IDs\n",
    "\n",
    "#   For  Neural Networks -\n",
    "#   Determine the highest prediction %\n",
    "#   on each row  for the final submission prediction\n",
    "# submissionDF['status_group'] = np.argmax(predictions, axis=-1 )\n",
    "\n",
    "#  XGBoost returns the target  as a single number per X instance\n",
    "#\n",
    "submissionDF['status_group'] = predictions\n",
    "\n",
    "# stats\n",
    "#\n",
    "positiveList = np.where(submissionDF['status_group'] > 0)\n",
    "positiveCount = np.array( positiveList )\n",
    "print(\"Prediction POSITIVITY Count == \", positiveCount.size, \\\n",
    "      \"  Rate == \", ( positiveCount.size / len(submissionDF) ) )\n",
    "\n",
    "#    Test random entries for Baseline (Accuracy== .3318  on DD )\n",
    "#       submissionDF['status_group'] =random.randint(3, size=(len(predictions)) )\n",
    "\n",
    "# Return the numbers to strings per the submission rules\n",
    "submissionDF['status_group'].replace([0, 1, 2], ['functional', 'functional needs repair', 'non functional'],\n",
    "                         inplace=True)\n",
    "\n",
    "# Write the actual file to disk\n",
    "submissionDF.to_csv( \"tanzania_submission.csv\", index=False )\n",
    "print( \"\\nWrote Submission file (tanzania_submission.csv) with Shape == \", submissionDF.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a2229",
   "metadata": {},
   "source": [
    "#  Mission accomplished!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7c3507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####   Tanzania Water Pump Predictions - Total EXECUTION Time = 0:01:04.493564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mission Complete!\n",
    "##################################################################################\n",
    "global_later = datetime.now()\n",
    "print(\"#####   Tanzania Water Pump Predictions - Total EXECUTION Time =\", (global_later - global_now) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mlp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
